# Daphne Project
Theoretically conscious Artifical Intelligence (with emotions), has builtin methods for generic robotics and sight, sense and hearing devices when provided correctly.
> [!CAUTION]
> This project is solely driven by the moral of education. It will stay closed-source due to certain unwanted series of events. After some refactoring of the code and more experiments (like turing tests), we *might* release it to the public.

### How does it work?
The algorithm behind it does not focus on how neurons work, rather than it replicates the basic functionality of a human brain. It does have ML algorithms for the memory store-recall-modify operations giving a quite similar behaviour like how humans learn. But humans can learn things on their own because we have one thing that is different than a normal ML or AI we know today and that is **Logic Unit**, as it's a part of a human brain's Prefrontal Lobe.

The prefrontal cortex is primarily responsible for the 'higher' brain functions of the frontal lobes, including decision-making, problem-solving, intelligence, and emotion regulation. The frontal cortex has also been shown to be activated when an experience becomes conscious. Different ideas and perceptions are bound together in this region, both of which are necessary for conscious experience. Concluding that this area may be especially important for consciousness. Bound to these facts, we will get onto the next implementation, **Consciousness**.

Consciousness can be explained in different terms like "a type of awareness of internal and external existence" and "a feeling provided by free-will that makes us aware of our own free-will". I see consciousness as **"a work of both logic-unit and emotions that provides a decision between a logical analysis and an emotional analysis"**. We can simulate this behavior by recalling certain stored memories and current emotional status. Emotion can lead to different things like when happy you are most likely to pick yes on something that is either gives no harm or provides something good either temporarily or for a long time, vice versa for the anger. Logic will provide the best-case scenario for the survival, power and bliss. When these two collide, a random pick can be given. But one side will get over onto the other. As an example, let's say that you want to try bungee jumping, you might get afraid that it can hurt you but you also want to try it because you are curious, later on you decide not to because "you simply chose not to". That choice is actually not your logic but your emotions because those emotions made you think that "you should not risk it and you will be using excess amount of energy for it anyway."

For emotions, we can parameterize them since everything is going to be internal, not like a human's bodily functions (Brain depending on other organs to release hormones etc.) and depending on the parameters, we can increase certain behaviours and reduce other functions to provide a *synthetic* emotion behavior.

For logic, we will hard-code everything that we know at the literal birth of our human brain. Things like `If pain detected, it is BAD` and more. This question arises when implementing this behavior: **"How does this brain will know the pain itself?"**. There is a simple answer. *Pain is just a couple overdriven neuron signals.*

### Proof that this project is real?
All I can provide is just the tree-view of the source files.

![image](https://github.com/user-attachments/assets/4af9a555-9fee-4a87-a904-aa6d83870726)
